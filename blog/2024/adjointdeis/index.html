<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Continuous Adjoint Equations for Diffusion Models | Zander W. Blasingame </title> <meta name="author" content="Zander W. Blasingame"> <meta name="description" content="This blog introduces the topic of the continuous adjoint equations for diffusion models, an efficient way to calculate gradients for diffusion models. We show how to design bespoke ODE/SDE solvers of the continuous adjoint equations and show that adjoint diffusion SDEs actually simplify to the adjoint diffusion ODE."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.ico?af5d64a855aa289ac39aca7ca5490cf6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://zblasingame.github.io/blog/2024/adjointdeis/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "The Continuous Adjoint Equations for Diffusion Models",
            "description": "This blog introduces the topic of the continuous adjoint equations for diffusion models, an efficient way to calculate gradients for diffusion models. We show how to design bespoke ODE/SDE solvers of the continuous adjoint equations and show that adjoint diffusion SDEs actually simplify to the adjoint diffusion ODE.",
            "published": "November 20, 2024",
            "authors": [
              
              {
                "author": "Zander W. Blasingame",
                "authorURL": "https://zblasingame.github.io/",
                "affiliations": [
                  {
                    "name": "Clarkson University",
                    "url": "https://camel.clarkson.edu/"
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Zander</span> W. Blasingame </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/news/">News </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>The Continuous Adjoint Equations for Diffusion Models</h1> <p>This blog introduces the topic of the continuous adjoint equations for diffusion models, an efficient way to calculate gradients for diffusion models. We show how to design bespoke ODE/SDE solvers of the continuous adjoint equations and show that adjoint diffusion SDEs actually simplify to the adjoint diffusion ODE.</p> </d-title> <d-byline></d-byline> <d-article> <d-contents> <nav class="l-text figcaption"> <h3>Contents</h3> <div> <a href="#introduction">Introduction</a> </div> <div> <a href="#diffusion-models">Diffusion models</a> </div> <ul> <li> <a href="#reversing-the-diffusion-sde">Reversing the diffusion SDE</a> </li> <li> <a href="#probability-flow-ode">Probability Flow ODE</a> </li> </ul> <div> <a href="#guided-generation-for-diffusion-models">Guided generation for diffusion models</a> </div> <div> <a href="#continuous-adjoint-equations-for-diffusion-models">Continuous adjoint equations for diffusion models</a> </div> <ul> <li> <a href="#simplified-formulation">Simplified formulation</a> </li> <li> <a href="#numerical-solvers">Numerical solvers</a> </li> <li> <a href="#implementation">Implementation</a> </li> <li> <a href="#adjoint-diffusion-sdes-are-actually-odes">Adjoint diffusion SDEs are actually ODEs</a> </li> </ul> <div> <a href="#concluding-remarks">Concluding Remarks</a> </div> </nav> </d-contents> <div style="display:none"> $$ \newcommand{\R}{\mathbb{R}} \newcommand{\X}{\mathcal{X}} \newcommand{\Y}{\mathcal{Y}} \newcommand{\B}{\mathcal{B}} \newcommand{\T}{\mathbb{T}} \newcommand{\N}{\mathbb{N}} \newcommand{\Z}{\mathcal{Z}} \newcommand{\Q}{\mathbb{Q}} \newcommand{\pr}{\mathbb{P}} \newcommand{\bfx}{\mathbf{x}} \newcommand{\bfy}{\mathbf{y}} \newcommand{\bfz}{\mathbf{z}} \newcommand{\bfa}{\mathbf{a}} \newcommand{\bfw}{\mathbf{w}} \newcommand{\bfA}{\mathbf{A}} \newcommand{\bfV}{\mathbf{V}} \newcommand{\bsf}{\boldsymbol{f}} \newcommand{\bsg}{\boldsymbol{g}} \newcommand{\bseps}{\boldsymbol{\epsilon}} \newcommand{\rmd}{\mathrm{d}} \DeclareMathOperator{\var}{Var} \DeclareMathOperator{\ex}{\mathbb{E}} \DeclareMathOperator{\argmax}{arg\,max} \DeclareMathOperator{\argmin}{arg\,min} \newtheorem{proposition}{Proposition} $$ </div> <h2 id="introduction">Introduction</h2> <p>Guided generation is an important problem problem within machine learning. Solutions to this problem enable us to steer the output of the generative process to some desired output. This is especially important for allowing us to inject creative control into generative models. While there are several forms of this problem, we focus on problems which optimize the output of generative model towards some goal defined by a guidance (or loss) function defined on the output. These particular approaches excel in steering the generative process to perform <a href="https://en.wikipedia.org/wiki/Adversarial_machine_learningi" rel="external nofollow noopener" target="_blank">adversarial ML</a> attacks, <em>e.g.</em>, bypassing security features, attacking Face Recognition (FR) systems, <em>&amp;c.</em></p> <p>More formally, suppose we have some \(\R^d\) generative model, \(\bsg_\theta: \R^z \times \R^c \to \R^d\) parameterized by \(\theta \in \R^m\) which takes an initial latent \(\bfz \in \R^z\) and conditional information \(\mathbf{c} \in \R^c\). Furthermore, assume we have a scalar-valued guidance function \(\mathcal{L}: \R^d \to \R\). Then the guided generation problem can be expressed as an optimization problem: \begin{equation} \label{eq:opt_init} \argmin_{\bfz, \mathbf{c}, \theta} \quad \mathcal{L}(\bsg_\theta(\bfz, \mathbf{c})). \end{equation} <em>I.e.</em>, we wish to find the optimal \(\bfz\), \(\mathbf{c}\), and \(\theta\) which minimizes our guidance function. A very natural solution to this kind of problem is to perform <a href="https://en.wikipedia.org/wiki/Gradient_descent" rel="external nofollow noopener" target="_blank">gradient descent</a> by using <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="external nofollow noopener" target="_blank">reverse-mode automatic differentiation</a> to find the gradients.</p> <p>In this blog post, we focus on a technique for finding the gradients for a very popular class of generative models known as <em>diffusion models</em> <d-cite key="song2021denoising,ddpm"></d-cite> by solving the <em>continuous adjoint equations</em> <d-cite key="kidger_thesis"></d-cite>.</p> <h2 id="diffusion-models">Diffusion models</h2> <p>First we give a brief introduction on diffusion models and score-based generative modeling. More comprehensive coverage can be found at <a href="https://yang-song.net/blog/2021/score/" rel="external nofollow noopener" target="_blank">Yang Song’s blog post</a> and <a href="https://lilianweng.github.io/posts/2021-07-11-diffusion-models/" rel="external nofollow noopener" target="_blank">Lilian Weng’s blog post</a> on this topic.</p> <p>Diffusion models start with a diffusion process which perturbs the original data distribution \(p_{\textrm{data}}(\bfx)\) on \(\R^d\) into isotropic Gaussian noise \(\mathcal{N}(\mathbf 0, \mathbf I)\). This process can be modeled with an Itô <a href="https://en.wikipedia.org/wiki/Stochastic_differential_equation" rel="external nofollow noopener" target="_blank">Stochastic Differential Equation</a> (SDE) of the form \begin{equation} \label{eq:ito_diffusion} \mathrm{d}\bfx_t = \underbrace{f(t)\bfx_t\; \mathrm dt}_{\textrm{Deterministic term $\approx$ an ODE}} + \underbrace{g(t)\; \mathrm d\mathbf{w}_t,}_{\textrm{Stochastic term}} \end{equation} where \(f, g\) are real-valued functions, \(\{\bfw_t\}_{t \in [0, T]}\) is the standard <a href="https://en.wikipedia.org/wiki/Wiener_process" rel="external nofollow noopener" target="_blank">Wiener process</a> on time \([0, T]\), and \(\mathrm d\bfw_t\) can be thought of as infinitesimal white noise. The drift coefficient \(f(t)\bfx_t\) is the deterministic part of the SDE and \(f(t)\bfx_t\;\mathrm dt\) can be thought of as the <a href="https://en.wikipedia.org/wiki/Ordinary_differential_equation" rel="external nofollow noopener" target="_blank">ODE</a> term of the SDE. Conversely, the diffusion coefficient \(g(t)\) is the stochastic part of the SDE which controls how much noise is injected into the system. </p> <p>The solution to this SDE is a continuous collection of random variables \(\{\bfx_t\}_{t \in [0, T]}\) over the real interval \([0, T]\), these random variables trace stochastic trajectories over the time interval. Let \(p_t(\bfx_t)\) denote the marginal <a href="https://en.wikipedia.org/wiki/Probability_density_function" rel="external nofollow noopener" target="_blank">probability density function</a> of \(\bfx_t\). Then \(p_0(\bfx_0) = p_{\textrm{data}}(\bfx)\) is the data distribution, likewise, for some sufficiently large \(T \in \R\) the terminal distribution \(p_T(\bfx_T)\) is <em>close</em> to some tractable noise distribution \(\pi(\bfx)\), called the <strong>prior distribution</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/dim/diffusion_overview-480.webp 480w,/assets/img/dim/diffusion_overview-800.webp 800w,/assets/img/dim/diffusion_overview-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/dim/diffusion_overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of diffusion SDE. Original clean image (left) is slowly perturbed by additions of white noise until there is only noise (right). To sample a clean image from a noisy image we only need to solve the SDE in reverse-time, see Equation \eqref{eq:rev_sde}. </div> <h3 id="reversing-the-diffusion-sde">Reversing the diffusion SDE</h3> <p>So far we have only covered how to destroy data by perturbing it with white noise, however, for sampling we need to be able reverse this process to <em>create</em> data from noise. Remarkably, Anderson <d-cite key="reverse_time_sdes"></d-cite> showed that the Itô SDE in Equation \eqref{eq:ito_diffusion} has a corresponding reverse SDE given in closed form by \begin{equation} \label{eq:rev_sde} \mathrm d\bfx_t = [f(t)\bfx_t - g^2(t)\underbrace{\nabla_\bfx\log p_t(\bfx_t)}_{\textrm{Score function}}]\;\mathrm dt + g(t)\; \mathrm d\bar\bfw_t, \end{equation} where \(\rmd t\) denotes a <em>negative</em> infinitesmial timestep and \(\nabla_\bfx\log p_t(\bfx_t)\) denotes the <strong>score function</strong> of \(p_t(\bfx_t)\). Note, the stochastic term is now driven by a <em>different</em> Wiener process defined on the backwards flow of time, <em>i.e.</em>, \(\bar\bfw_T = \mathbf 0\) a.s.<d-footnote>For technical reasons we use the abbreviation a.s. which denotes almost surely, <i>i.e.</i>, an event happens <i>almost surely</i> if it happens with probability 1. </d-footnote> For a modern derivation of Anderson’s result we recommend checking out the excellent blog post by <a href="https://ludwigwinkler.github.io/blog/ReverseTimeAnderson/" rel="external nofollow noopener" target="_blank">Ludwig Winkler on this topic</a>.</p> <p>To train a diffusion model, then, we just need to learn the score function via score-matching <d-cite key="song2021scorebased"></d-cite> or some closely related quantity like the added noise or \(\bfx_0\)-prediction <d-cite key="ddpm,progressive_distillation"></d-cite>. Many diffusion models use the following choice of drift and diffusion coefficients: \begin{equation} f(t) = \frac{\mathrm d \log \alpha_t}{\mathrm dt},\qquad g^2(t)= \frac{\mathrm d \sigma_t^2}{\mathrm dt} - 2 \frac{\mathrm d \log \alpha_t}{\mathrm dt} \sigma_t^2. \end{equation} Where \(\alpha_t,\sigma_t\) form a noise schedule such that \(\alpha_t^2 + \sigma_t^2 = 1\) and \begin{equation} \bfx_t = \alpha_t\bfx_0 + \sigma_t\boldsymbol\epsilon_t \qquad \boldsymbol\epsilon_t \sim \mathcal{N}(\mathbf 0, \mathbf I). \end{equation} Diffusion models which use noise prediction train a neural network \(\boldsymbol\epsilon_\theta(\bfx_t, t)\) parameterized by \(\theta\) to predict \(\boldsymbol\epsilon_t\) given \(\bfx_t\) which is equivalent to learning \(\boldsymbol\epsilon_\theta(\bfx_t, t) \approx -\sigma_t\nabla_\bfx \log p_t(\bfx_t)\). This choice of drift and coefficients form the Variance Preserving type SDE (VP type SDE) <d-cite key="song2021scorebased"></d-cite>.</p> <h3 id="probability-flow-ode">Probability Flow ODE</h3> <p>Song <em>et al.</em> <d-cite key="song2021scorebased"></d-cite> showed the existence of an ODE, dubbed the <em>Probability Flow</em> ODE, whose trajectories have the same marginals as Equation \eqref{eq:rev_sde} of the form \begin{equation} \label{eq:pf_ode} \frac{\mathrm d\bfx_t}{\mathrm dt} = f(t)\bfx_t - \frac 12 g^2(t) \nabla_\bfx \log p_t(\bfx_t). \end{equation} <em>N.B.</em>, this form can be found by following the derivation used by Anderson <d-cite key="reverse_time_sdes"></d-cite> and manipulating <a href="https://en.wikipedia.org/wiki/Kolmogorov_equations" rel="external nofollow noopener" target="_blank">Kolmogorov equations</a> to write a reverse-time SDE with \(\mathbf 0\) for the diffusion coefficient, <em>i.e.</em>, an ODE.</p> <p>One of key benefits of expressing diffusion models in ODE form is that ODEs are easily reversible, by simply integrating forwards and backwards in time we can encode images from \(p_0(\bfx_0)\) into \(p_T(\bfx_T)\) and back again. With a neural network, often a U-Net <d-cite key="unet"></d-cite>, \(\boldsymbol\epsilon_\theta(\bfx_t, t)\) trained on noise prediction the <em>empirical Probability Flow</em> ODE is now \begin{equation} \label{eq:empirical_pf_ode} \frac{\mathrm d\bfx_t}{\mathrm dt} = f(t)\bfx_t + \frac{g^2(t)}{2\sigma_t} \boldsymbol\epsilon_\theta(\bfx_t, t). \end{equation}</p> <h2 id="guided-generation-for-diffusion-models">Guided generation for diffusion models</h2> <p>Researchers have proposed many ways to perform guided generation with diffusion models. Outside of directly conditioning the noise-prediction network on additional latent information Dhariwal and Nichol proposed classifier guidance <d-cite key="diff_beat_gan"></d-cite> which uses an external classifier \(p(\bfz|\bfx)\) is used to augment the score function \(\nabla_\bfx \log p_t(\bfx_t|\bfz)\). Later work, by Ho and Salimans <d-cite key="ho2021classifierfree"></d-cite> showed the classifier could be omitted by incorporating the conditional information in training with the following parameterization of the noise-prediction model \begin{equation} \tilde{\boldsymbol\epsilon}_\theta(\bfx_t, \bfz, t) := \gamma \boldsymbol\epsilon_\theta(\bfx_t, \bfz, t) + (1 - \gamma) \boldsymbol\epsilon_\theta(\bfx_t, \mathbf 0, t), \end{equation} where $\gamma \geq 0$ is the guidance scale.</p> <p>Outside of methods which require the additional to the diffusion model, or some external network, there are <strong>training-free methods</strong> which we broadly categorize into the following two categories:</p> <ol> <li>Techniques which directly optimize the solution trajectory during sampling <d-cite key="yu2023freedom,greedy_dim,liu2023flowgrad"></d-cite>.</li> <li>Techniques which search for the optimal generation parameters, <em>e.g.</em>, \((\bfx_T, \bfz, \theta)\), (this can include optimizing the solution trajectory as well) <d-cite key="doodl,pan2024adjointdpm,adjointdeis,marion2024implicit"></d-cite>.</li> </ol> <p>The second solution of techniques is related to our initial problem statement in the introduction from Equation \eqref{eq:opt_init}. We reframe this problem for the specific case of diffusion ODEs.</p> <p><strong>Problem statement.</strong> Given the diffusion ODE in Equation \eqref{eq:empirical_pf_ode}, we wish to solve the following optimization problem: \begin{equation} \label{eq:problem_stmt_ode} \argmin_{\bfx_T, \bfz, \theta}\quad \mathcal{L}\bigg(\bfx_T + \int_T^0 f(t)\bfx_t + \frac{g^2(t)}{2\sigma_t}\bseps_\theta(\bfx_t, \bfz, t)\;\rmd t\bigg). \end{equation} <em>N.B.</em>, without loss of generality we let \(\bseps_\theta(\bfx_t, \bfz, t)\) denote a noise-prediction network conditioned either directly on $\bfz$ or as the classifier-free guidance model \(\tilde \bseps_\theta(\bfx_t, \bfz, t)\).</p> <p>From this formulation it is readily apparent the difficulty introduced by diffusion models, over say other methods like <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network" rel="external nofollow noopener" target="_blank">GANs</a> or <a href="https://en.wikipedia.org/wiki/Variational_autoencoder" rel="external nofollow noopener" target="_blank">VAEs</a>, is that we need to perform backpropagation through an ODE solve. Luckily, diffusion models are a type of Neural ODE <d-cite key="neural_ode"></d-cite> which means we can solve the <em>continuous adjoint equations</em> to calculate these gradients.</p> <h2 id="continuous-adjoint-equations-for-diffusion-models">Continuous adjoint equations for diffusion models</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/adjointdeis/overview-480.webp 480w,/assets/img/adjointdeis/overview-800.webp 800w,/assets/img/adjointdeis/overview-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/adjointdeis/overview.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Overview of solving the continuous adjoint equations with diffusion models. The sampling schedule consists of $\{t_n\}_{n=0}^N$ timesteps for the diffusion model and $\{\tilde t_n\}_{n=0}^M$ timesteps for AdjointDEIS. The gradients $\bfa_\bfx(T)$ can be used to optimize $\bfx_T$ to find some optimal $\bfx_T^*$. In this example we use the solver known as AdjointDEIS <d-cite key="adjointdeis"></d-cite>. </div> <p>The technique of solving an <em>adjoint</em> backwards-in-time ODE to calculate the gradients of an ODE is widely used and widespread technique initially proposed by Pontryagin <em>et al.</em> <d-cite key="adjoint_sensitivity_method"></d-cite>. The technique was recently popularized in the ML community by Chen <em>et al.</em> <d-cite key="neural_ode"></d-cite> in their seminal work on Neural ODEs with extensions to other models such as SDEs <d-cite key="adjointsde,kidger_thesis"></d-cite>.</p> <p>We can write the diffusion ODE as a Neural ODE of the form: \(\begin{equation} \frac{\rmd \bfx_t}{\rmd t} = \bsf_\theta(\bfx_t, \bfz, t) := f(t)\bfx_t + \frac{g^2(t)}{2\sigma_t}\bseps(\bfx_t, \bfz, t). \end{equation}\) Then \(\bsf_\theta(\bfx_t, \bfz, t)\) and assuming \(\bsf_\theta\) is continuous in $t$ and uniformly Lipschitz in \(\bfx\),<d-footnote>These conditions are to ensure that the Picard-Lindelöf theorem holds guaranteeing a unique solution to the IVP. </d-footnote> then \(\bsf_\theta(\bfx_t, \bfz, t)\) describes a neural ODE which has a unique solution with the initial condition \(\bfx_T\) and admits an adjoint state \(\bfa_\bfx(t) := \partial\mathcal{L} / \partial \bfx_t\) (and likewise for \(\bfa_\bfz(t)\) and \(\bfa_\theta(t)\)), which solve the continuous adjoint equations, see Theorem 5.2 in <d-cite key="kidger_thesis"></d-cite>, in the form of the <a href="https://en.wikipedia.org/wiki/Initial_value_problem" rel="external nofollow noopener" target="_blank">Initial Value Problem</a> (IVP): \(\begin{align} \bfa_{\bfx}(0) &amp;= \frac{\partial \mathcal{L}}{\partial \bfx_0}, \qquad &amp;&amp; \frac{\rmd \bfa_{\bfx}}{\rmd t}(t) = -\bfa_{\bfx}(t)^\top \frac{\partial \bsf_\theta(\bfx_t, \bfz, t)}{\partial \bfx_t},\nonumber \\ \bfa_{\bfz}(0) &amp;= \mathbf 0, \qquad &amp;&amp; \frac{\rmd \bfa_{\bfz}}{\rmd t}(t) = -\bfa_{\bfx}(t)^\top \frac{\partial \bsf_\theta(\bfx_t, \bfz, t)}{\partial \bfz},\nonumber \\ \bfa_{\theta}(0) &amp;= \mathbf 0, \qquad &amp;&amp; \frac{\rmd \bfa_{\theta}}{\rmd t}(t) = -\bfa_{\bfx}(t)^\top \frac{\partial \bsf_\theta(\bfx_t, \bfz, t)}{\partial \theta}. \label{eq:adjoint_ode} \end{align}\) We call this augmented ODE system an adjoint diffusion ODE. The adjoint state models the following gradients:</p> <ul> <li>\(\bfa_\bfx(t) := \partial \mathcal L / \partial \bfx_t\). The gradient of the guidance function w.r.t. solution trajectory at any time \(t\).</li> <li>\(\bfa_\bfz(T) := \partial \mathcal L / \partial \bfz\). The gradient of the guidance function w.r.t. the model conditional information.</li> <li>\(\bfa_\theta(T) := \partial \mathcal L / \partial \theta\). The gradient of the guidance function w.r.t. the model parameters.</li> </ul> <p>While this formulation can calculate the desired gradients to solve the optimization problem it, however, fails to account of the unique construction of diffusion models in particular the special formulation of \(f\) and \(g\). Recent work <d-cite key="pan2024adjointdpm,adjointdeis"></d-cite> has shown that by taking this construction into consideration the adjoint diffusion ODE can be considerably simplified enabling the creation of efficient solvers for the continuous adjoint equations.</p> <details><summary>Note on the flow of time</summary> <p>In the literature of diffusion models the sampling process is often done in reverse-time, <em>i.e.</em>, the initial noise is \(\bfx_T\) and the final sample is \(\bfx_0\). Due to this convention solving the adjoint diffusion ODE <em>backwards</em> actually means integrating <em>forwards</em> in time. Thus while diffusion models learn to compute \(\bfx_t\) from \(\bfx_s\) with \(s &gt; t\), the adjoint diffusion ODE seeks to compute \(\bfa_\bfx(s)\) from \(\bfa_\bfx(t)\).</p> </details> <h3 id="simplified-formulation">Simplified formulation</h3> <p>Recent work on efficient ODE solvers for diffusion models <d-cite key="dpm_solver,deis_georgiatech"></d-cite> have shown that by using <em>exponential integrators</em> <d-cite key="exponential_integrators"></d-cite> diffusion ODEs can be simplified and the error in the linear term removed entirely. Likewise, <d-cite key="pan2024adjointdpm,adjointdeis"></d-cite> showed that this same property follows for adjoint diffusion ODEs.</p> <p>The continuous adjoint equation for \(\bfa_\bfx(t)\) in Equation \eqref{eq:adjoint_ode} can be rewritten as \(\begin{equation} \label{eq:empirical_adjoint_ode} \frac{\mathrm d\bfa_\bfx}{\mathrm dt}(t) = -f(t)\bfa_\bfx(t) - \frac{g^2(t)}{2\sigma_t}\bfa_\bfx(t)^\top \frac{\partial \bseps_\theta(\bfx_t, \bfz, t)}{\partial \bfx_t}. \end{equation}\)</p> <p>Due to the gradient of the drift term in Equation \eqref{eq:empirical_adjoint_ode}, further manipulations are required to put the empirical adjoint probability flow ODE into a sufficiently ``nice’’ form. We can transform this <a href="https://en.wikipedia.org/wiki/Stiff_equation" rel="external nofollow noopener" target="_blank">stiff ODE</a> into a non-stiff form by applying the integrating factor \(\exp\big({\int_0^t f(\tau)\;\mathrm d\tau}\big)\) to Equation \eqref{eq:empirical_adjoint_ode}, which is expressed as: \(\begin{equation} \label{eq:empirical_adjoint_ode_IF} \frac{\mathrm d}{\mathrm dt}\bigg[e^{\int_0^t f(\tau)\;\mathrm d\tau} \bfa_\bfx(t)\bigg] = -e^{\int_0^t f(\tau)\;\mathrm d\tau} \frac{g^2(t)}{2\sigma_t}\bfa_\bfx(t)^\top \frac{\partial \bseps_\theta(\bfx_t, \bfz, t)}{\partial \bfx_t}. \end{equation}\) Then, the exact solution at time \(s\) given time \(t &lt; s\) is found to be \(\begin{align} \bfa_\bfx(s) = \underbrace{\vphantom{\int_t^s}e^{\int_s^t f(\tau)\;\mathrm d\tau} \bfa_\bfx(t)}_{\textrm{linear}} - \underbrace{\int_t^s e^{\int_s^u f(\tau)\;\mathrm d\tau} \frac{g^2(u)}{2\sigma_u} \bfa_\bfx(u)^\top \frac{\bseps_\theta(\bfx_u, \bfz, u)}{\partial \bfx_u}\;\rmd u}_{\textrm{non-linear}}. \label{eq:empirical_adjoint_ode_x} \end{align}\) With this transformation we can compute the linear in closed form, thereby <strong>eliminating</strong> the discretization error in the linear term. However, we still need to approximate the non-linear term which consists of a difficult integral about the complex noise-prediction model. This is where the insight of Lu <em>et al.</em> <d-cite key="dpm_solver"></d-cite> to integrate in the log-SNR domain becomes invaluable. Let \(\lambda_t := \log(\alpha_t/\sigma_t)\) be one half of the log-SNR. Then, with using this new variable and computing the drift and diffusion coefficients in closed form, we can rewrite Equation \eqref{eq:empirical_adjoint_ode_x} as \(\begin{equation} \label{eq:empirical_adjoint_ode_x2} \bfa_\bfx(s) = \frac{\alpha_t}{\alpha_s} \bfa_\bfx(t) + \frac{1}{\alpha_s}\int_t^s \alpha_u\sigma_u \frac{\rmd \lambda_u}{\rmd u} \bfa_\bfx(u)^\top \frac{\bseps_\theta(\bfx_u, \bfz, u)}{\partial \bfx_u}\;\rmd u. \end{equation}\) As \(\lambda_t\) is a strictly decreasing function w.r.t. \(t\) it therefore has an inverse function \(t_{\lambda}\) which satisfies \(t_{\lambda}(\lambda_t) = t\), and, with abuse of notation, we let \(\bfx_{\lambda} := \bfx_{t_\lambda(\lambda)}\), \(\bfa_\bfx(\lambda) := \bfa_\bfx(t_{\lambda}(\lambda))\), <em>&amp;c.</em> and let the reader infer from context if the function is mapping the log-SNR back into the time domain or already in the time domain. Then by rewriting Equation \eqref{eq:empirical_adjoint_ode_x2} as an exponentially weighted integral and performing an analogous derivation for \(\bfa_\bfz(t)\) and \(\bfa_\theta(t)\), we arrive at:</p> <p><strong>Proposition</strong> <em>(Exact solution of adjoint diffusion ODEs)</em><strong>.</strong> Given initial values \([\bfa_\bfx(t), \bfa_\bfz(t), \bfa_\theta(t)]\) at time \(t \in (0,T)\), the solution \([\bfa_\bfx(s), \bfa_\bfz(s), \bfa_\theta(s)]\) at time \(s \in (t, T]\) of adjoint diffusion ODEs in Equation \eqref{eq:adjoint_ode} is \(\begin{align} \label{eq:exact_sol_empirical_adjoint_ode_x} \bfa_\bfx(s) &amp;= \frac{\alpha_t}{\alpha_s} \bfa_\bfx(t) + \frac{1}{\alpha_s}\int_{\lambda_t}^{\lambda_s} \alpha_\lambda^2 e^{-\lambda} \bfa_\bfx(\lambda)^\top \frac{\partial \bseps_\theta(\bfx_\lambda, \bfz, \lambda)}{\partial \bfx_\lambda}\;\rmd \lambda,\\ \label{eq:exact_sol_empirical_adjoint_ode_z} \bfa_\bfz(s) &amp;= \bfa_\bfz(t) + \int_{\lambda_t}^{\lambda_s}\alpha_\lambda e^{-\lambda} \bfa_\bfx(\lambda)^\top \frac{\partial \boldsymbol\epsilon_\theta(\bfx_\lambda, \bfz, \lambda)}{\partial \bfz}\;\rmd\lambda,\\ \label{eq:exact_sol_empirical_adjoint_ode_theta} \bfa_\theta(s) &amp;= \bfa_\theta(t) + \int_{\lambda_t}^{\lambda_s}\alpha_\lambda e^{-\lambda} \bfa_\bfx(\lambda)^\top \frac{\partial \boldsymbol\epsilon_\theta(\bfx_\lambda, \bfz, \lambda)}{\partial \theta}\;\rmd\lambda. \end{align}\)</p> <h3 id="numerical-solvers">Numerical solvers</h3> <p>Now that we have a simplified formulation of the continuous adjoint equations we can construct bespoke numerical solvers. To do this we take approximate the integral term via a Taylor expansion which we illustrate for Equation \eqref{eq:exact_sol_empirical_adjoint_ode_x}.For \(k \geq 1\) a \((k-1)\)-th Taylor expansion of the scaled vector Jacobian about \(\lambda_t\) is equal to</p> <div class="l-body-outset"> $$\begin{equation} \alpha_\lambda^2\bfa_\bfx(\lambda)^\top \frac{\partial \boldsymbol\epsilon_\theta(\bfx_\lambda, \bfz, \lambda)}{\partial \bfx_\lambda} = \sum_{n=0}^{k-1} \frac{(\lambda - \lambda_t)^n}{n!} \frac{\mathrm d^n}{\mathrm d\lambda^n}\bigg[\alpha_\lambda^2\bfa_\bfx(\lambda)^\top \frac{\partial \boldsymbol\epsilon_\theta(\bfx_\lambda, \bfz, \lambda)}{\partial \bfx_\lambda}\bigg]_{\lambda = \lambda_t} + \mathcal{O}((\lambda - \lambda_t)^k). \end{equation}$$ </div> <p>For notational convenience we denote the $n$-th order derivative of scaled vector-Jacobian products at \(\lambda_t\) as \(\begin{equation} \label{eq:app:vjp_def_x} \bfV^{(n)}(\bfx; \lambda_t) = \frac{\rmd^n}{\rmd \lambda^n}\bigg[\alpha_\lambda^2\bfa_\bfx(\lambda)^\top \frac{\partial \bseps_\theta(\bfx_\lambda, \bfz, \lambda)}{\partial \bfx_\lambda}\bigg]_{\lambda = \lambda_t}. \end{equation}\) Then substituting our Taylor expansion into Equation \eqref{eq:exact_sol_empirical_adjoint_ode_x} and letting \(h = \lambda_s - \lambda_t\) denote the step size we have a \(k\)-th order solver for the continuous adjoint equation for \(\bfa_\bfx(t)\):</p> <div class="l-body-outset"> $$\begin{equation} \bfa_\bfx(s) = \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t) }_{\substack{\textrm{Linear term}\\\textbf{Exactly computed}}} +\frac{1}{\alpha_s} \sum_{n=0}^{k-1} \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \bfV^{(n)}(\bfx; \lambda_t) }_{\substack{\textrm{Derivatives}\\\textbf{Approximated}}}\; \underbrace{ \int_{\lambda_t}^{\lambda_s} \frac{(\lambda - \lambda_t)^n}{n!} e^{-\lambda}\;\mathrm d\lambda }_{\substack{\textrm{Coefficients}\\\textbf{Analytically computed}}} + \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \mathcal{O}(h^{k+1}). }_{\substack{\textrm{Higher-order errors}\\\textbf{Omitted}}} \end{equation}$$ </div> <p>Let’s break this down term by term.</p> <ol> <li> <p><strong>Linear term.</strong> The linear term of the adjoint diffusion ODE can be calculated exactly using ratio of the signal schedule \(\alpha_t / \alpha_s\). As \(\alpha_t \geq \alpha_s\) for \(t \leq s\) this implies \(\alpha_t / \alpha_s \geq 1\). \(\begin{equation*} \bfa_\bfx(s) = {\color{orange}\underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t) }_{\substack{\textrm{Linear term}\\\textbf{Exactly computed}}}} +\frac{1}{\alpha_s} \sum_{n=0}^{k-1} \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \bfV^{(n)}(\bfx; \lambda_t) }_{\substack{\textrm{Derivatives}\\\textbf{Approximated}}}\; \underbrace{ \int_{\lambda_t}^{\lambda_s} \frac{(\lambda - \lambda_t)^n}{n!} e^{-\lambda}\;\mathrm d\lambda }_{\substack{\textrm{Coefficients}\\\textbf{Analytically computed}}} + \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \mathcal{O}(h^{k+1}). }_{\substack{\textrm{Higher-order errors}\\\textbf{Omitted}}} \end{equation*}\)</p> </li> <li> <p><strong>Derivatives.</strong> The \(n\)-th order derivatives of scaled vector-Jacobian product can be efficiently estimated using multi-step methods <d-cite key="atkinson2011numerical"></d-cite>. \(\begin{equation*} \bfa_\bfx(s) = \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t) }_{\substack{\textrm{Linear term}\\\textbf{Exactly computed}}} +\frac{1}{\alpha_s} \sum_{n=0}^{k-1} {\color{orange}\underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \bfV^{(n)}(\bfx; \lambda_t) }_{\substack{\textrm{Derivatives}\\\textbf{Approximated}}}}\; \underbrace{ \int_{\lambda_t}^{\lambda_s} \frac{(\lambda - \lambda_t)^n}{n!} e^{-\lambda}\;\mathrm d\lambda }_{\substack{\textrm{Coefficients}\\\textbf{Analytically computed}}} + \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \mathcal{O}(h^{k+1}). }_{\substack{\textrm{Higher-order errors}\\\textbf{Omitted}}} \end{equation*}\)</p> </li> <li> <p><strong>Coefficients.</strong> The exponentially weighted integral can be analytically computed in closed form. \(\begin{equation*} \bfa_\bfx(s) = \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t) }_{\substack{\textrm{Linear term}\\\textbf{Exactly computed}}} +\frac{1}{\alpha_s} \sum_{n=0}^{k-1} \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \bfV^{(n)}(\bfx; \lambda_t) }_{\substack{\textrm{Derivatives}\\\textbf{Approximated}}}\; {\color{orange}\underbrace{ \int_{\lambda_t}^{\lambda_s} \frac{(\lambda - \lambda_t)^n}{n!} e^{-\lambda}\;\mathrm d\lambda }_{\substack{\textrm{Coefficients}\\\textbf{Analytically computed}}}} + \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \mathcal{O}(h^{k+1}). }_{\substack{\textrm{Higher-order errors}\\\textbf{Omitted}}} \end{equation*}\)</p> </li> <li> <p><strong>Higher-order errors.</strong> The remaining higher-order error terms are discarded. If \(h^{k+1}\) is sufficiently small than these errors are negligible. \(\begin{equation*} \bfa_\bfx(s) = \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t) }_{\substack{\textrm{Linear term}\\\textbf{Exactly computed}}} +\frac{1}{\alpha_s} \sum_{n=0}^{k-1} \underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \bfV^{(n)}(\bfx; \lambda_t) }_{\substack{\textrm{Derivatives}\\\textbf{Approximated}}}\; \underbrace{ \int_{\lambda_t}^{\lambda_s} \frac{(\lambda - \lambda_t)^n}{n!} e^{-\lambda}\;\mathrm d\lambda }_{\substack{\textrm{Coefficients}\\\textbf{Analytically computed}}} + {\color{orange}\underbrace{ \vphantom{\int_{\lambda_t}^{\lambda_s}} \mathcal{O}(h^{k+1}). }_{\substack{\textrm{Higher-order errors}\\\textbf{Omitted}}}} \end{equation*}\)</p> </li> </ol> <details><summary>Computing the exponentially weighted integral</summary> <p>The exponentially weighted integral can be solved <strong>analytically</strong> by applying \(n\) times integration by parts <d-cite key="dpm_solver,exponential_integrators"></d-cite> such that \(\begin{equation} \label{eq:exponential_integral} \int_{\lambda_t}^{\lambda_s} e^{-\lambda} \frac{(\lambda - \lambda_t)^n}{n!}\;\mathrm d\lambda = \frac{\sigma_s}{\alpha_s} h^{n+1}\varphi_{n+1}(h), \end{equation}\) with special \(\varphi\)-functions <d-cite key="exponential_integrators"></d-cite>. These functions are defined as \(\begin{equation} \varphi_{n+1}(h) := \int_0^1 e^{(1-u)h} \frac{u^n}{n!}\;\mathrm du,\qquad\varphi_0(h) = e^h, \end{equation}\) which satisfy the recurrence relation \(\varphi_{k+1}(h) = (\varphi_{k}(h) - \varphi_k(0)) / h\) and have closed forms for $k = 1, 2$: \(\begin{align} \varphi_1(h) &amp;= \frac{e^h - 1}{h},\\ \varphi_2(h) &amp;= \frac{e^h - h - 1}{h^2}. \end{align}\)</p> </details> <p>From this construction there are only two-sources of error. The error in approximating the \(n\)-th order derivative of the vector-Jacobian and the higher-order errors. Therefore, as we long as we pick a sufficiently small step size, \(h\), and appropriate order, \(k\), we can achieve accurate (enough) estimates of the gradients. The derivations for the solvers of \(\bfa_\bfz(t)\) and \(\bfa_\theta(t)\) are omitted for brevity but follow an analogous derivation. The \(k\)-th order solvers resulting from this method are called <strong>AdjointDEIS-\(k\)</strong>. In <d-cite key="adjointdeis"></d-cite> we prove that that AdjointDEIS-\(k\) are \(k\)-th order solvers for \(k = 1, 2\).</p> <h3 id="implementation">Implementation</h3> <p>Consider the case when \(k=1\) then we have the following first-order solver.</p> <p><strong>AdjointDEIS-1.</strong> Given an initial augmented adjoint state \([\bfa_\bfx(t), \bfa_\bfz(t), \bfa_\theta(t)]\) at time \(t \in (0, T)\), the solution \([\bfa_\bfx(s), \bfa_\bfz(s), \bfa_\theta(s)]\) at time \(s \in (t, T]\) is approximated by \(\begin{align} \bfa_\bfx(s) &amp;= \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t) + \sigma_s (e^h - 1) \frac{\alpha_t^2}{\alpha_s^2}\bfa_\bfx(t)^\top \frac{\partial \bseps(\bfx_t, \bfz, t)}{\partial \bfx_t},\nonumber\\ \bfa_\bfz(s) &amp;= \bfa_\bfz(t) + \sigma_s (e^h - 1) \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t)^\top \frac{\partial \bseps(\bfx_t, \bfz, t)}{\partial \bfz},\nonumber\\ \bfa_\theta(s) &amp;= \bfa_\theta(t) + \sigma_s (e^h - 1) \frac{\alpha_t}{\alpha_s}\bfa_\bfx(t)^\top \frac{\partial \bseps(\bfx_t, \bfz, t)}{\partial \theta}. \label{eq:adjoint_deis_1_at} \end{align}\)</p> <p>The vector-Jacobian product can be easily calculated using reverse-mode automatic differentiation provided by most modern ML frameworks. We illustrate an implementation of this first-order solver using PyTorch. For simplicity we omit the code for calculating \(\bfa_\theta\) as it requires more boilerplate code.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="k">def</span> <span class="nf">adjointdeis_1</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">scheduler</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">guidance_function</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        model (torch.nn.Module): Noise prediction model takes `(x, z, t)` as inputs.
        scheduler: Object which manages the noise schedule and sampling solver for the diffusion model.
        x0 (torch.Tensor): Generated image `x0`.
        z (torch.Tensor): Conditional information.
        guidance_function: A scalar-valued guidance function which takes `x0` as input.
        timesteps (torch.Tensor): A sequence of strictly monotonically increasing timesteps.
    </span><span class="sh">"""</span>
    <span class="n">x0</span><span class="p">.</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">adjoint_x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="nf">guidance_function</span><span class="p">(</span><span class="n">x0</span><span class="p">).</span><span class="nf">mean</span><span class="p">(),</span> <span class="n">x0</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">adjoint_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
    <span class="n">xt</span> <span class="o">=</span> <span class="n">x0</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">timesteps</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">t</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">timesteps</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>

        <span class="n">model_out</span> <span class="o">=</span> <span class="nf">model</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span>

        <span class="c1"># Compute vector Jacobians
</span>        <span class="n">vec_J_xt</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">model_out</span><span class="p">,</span> <span class="n">xt</span><span class="p">,</span> <span class="n">adjoint_x</span><span class="p">,</span> <span class="n">retain_graph</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">vec_J_z</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">autograd</span><span class="p">.</span><span class="nf">grad</span><span class="p">(</span><span class="n">model_out</span><span class="p">,</span> <span class="n">z</span><span class="p">,</span> <span class="n">adjoint_z</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># Compute noise schedule parameters
</span>        <span class="n">lambda_t</span><span class="p">,</span> <span class="n">lambda_s</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="nf">lambda_t</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
        <span class="n">alpha_t</span><span class="p">,</span> <span class="n">alpha_s</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="nf">alpha_t</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>
        <span class="n">sigma_t</span><span class="p">,</span> <span class="n">sigma_s</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="nf">sigma_t</span><span class="p">([</span><span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">])</span>

        <span class="n">h</span> <span class="o">=</span> <span class="n">lambda_s</span> <span class="o">-</span> <span class="n">lambda_t</span>

        <span class="c1"># Solve AdjointDEIS-1
</span>        <span class="n">adjoint_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_t</span> <span class="o">/</span> <span class="n">alpha_s</span><span class="p">)</span> <span class="o">*</span> <span class="n">adjoint_x</span> <span class="o">+</span> <span class="n">sigma_s</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">expm1</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_t</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha_s</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">vec_J_xt</span>
        <span class="n">adjoint_z</span> <span class="o">=</span> <span class="n">adjoint_z</span> <span class="o">+</span> <span class="n">sigma_s</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">expm1</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_t</span> <span class="o">/</span> <span class="n">alpha_s</span><span class="p">)</span> <span class="o">*</span> <span class="n">vec_J_xt</span>

        <span class="c1"># Use some ODE solver to find next xt
</span>        <span class="n">xt</span> <span class="o">=</span> <span class="n">scheduler</span><span class="p">.</span><span class="nf">step</span><span class="p">(</span><span class="n">xt</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">s</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">xt</span><span class="p">,</span> <span class="n">adjoint_x</span><span class="p">,</span> <span class="n">adjoint_z</span></code></pre></figure> <h3 id="adjoint-diffusion-sdes-are-actually-odes">Adjoint diffusion SDEs are actually ODEs</h3> <p>What about diffusion SDEs, the problem statement in Equation \eqref{eq:problem_stmt_ode} would become \(\begin{equation} \label{eq:problem_stmt_sde} \argmin_{\bfx_T, \bfz, \theta}\quad \mathcal{L}\bigg(\bfx_T + \int_T^0 f(t)\bfx_t + \frac{g^2(t)}{\sigma_t}\bseps_\theta(\bfx_t, \bfz, t)\;\rmd t + \int_T^0 g(t) \; \rmd \bar\bfw_t\bigg). \end{equation}\) The technical details of working with SDEs are beyond the scope of this post; however, we will highlight one of the key insights from our work <d-cite key="adjointdeis"></d-cite>.</p> <p>Suppose we have an SDE in the <a href="https://en.wikipedia.org/wiki/Stratonovich_integral" rel="external nofollow noopener" target="_blank">Stratonovich</a> sense of the form \(\begin{equation} \label{eq:stratonovich_sde} \rmd \bfx_t = \bsf(\bfx_t, t)\;\rmd t + \bsg(t) \circ \rmd \bfw_t \end{equation}\) where \(\circ \rmd \bfw_t\) denotes integration in the Stratonovich sense and \(\bsf \in \mathcal{C}_b^{\infty, 1}(\R^d)\), <em>i.e.</em>, \(\bsf\) is continuous function to \(\R^d\) and has infinitely many bounded derivatives w.r.t. the state and bounded first derivatives w.r.t. to time. Likewise, let \(\bsg \in \mathcal{C}_b^1(\R^{d \times w})\) be a continuous function with bounded first derivatives. Lastly, let \(\bfw_t: [0,T] \to \R^w\) be a \(w\)-dimensional Wiener process. Then Equation \eqref{eq:stratonovich_sde} has unique strong solution given by \(\bfx_t: [0, T] \to \R^d\).</p> <p>We show in <d-cite key="adjointdeis"></d-cite> that the continuous adjoint equations of such an SDE reduce to a backwards-in-time SDE of the form \(\begin{equation} \label{eq:sde_is_ode} \rmd \bfa_\bfx(t) = -\bfa_\bfa(t)^\top \frac{\partial \bsf}{\partial \bfx_t}(\bfx_t, t)\;\rmd t \end{equation}\) with a \(\mathbf 0\) coefficient for the diffusion term and that there exists a unique strong solution to this SDE of the form \(\bfa_\bfx: [0,T] \to \R^d\). As the diffusion coefficient for this SDE is \(\mathbf 0\) then it is essentially an ODE. While glossing over some technical details this result should be straightforwardly apparent as the diffusion coefficient \(\bsg(t)\) relies only on time and not the state, nor other parameters of interest.</p> <p><strong>Remark.</strong> While the adjoint state evolves with an ODE the underlying state \(\bfx_t\) still evolves with a backwards-in-time SDE! This was the reason for our choice of Stratonovich over Itô as the Stratonovich integral is symmetric.</p> <p>Now our diffusion SDE can be easily converted into Stratonovich form due to the diffusion coefficient depending only on time. Moreover, due to the shared derivation using the Kolmogorov equations in constructing diffusion SDEs and diffusion ODEs, the two forms differ only by a factor of 2 within the drift term. \(\begin{equation} {\color{orange}\underbrace{\rmd \bfx_t = f(t)\bfx_t + {\color{black}2} \frac{g^2(t)}{2\sigma_t} \bseps_\theta(\bfx_t, \bfz, t)\;\rmd t}_{\textrm{Diffusion ODE}}} + g(t)\circ\rmd\bar\bfw_t. \end{equation}\) Furthermore, notice that SDE has form \(\begin{equation} \rmd \bfx_t = {\color{orange}\underbrace{f(t)\bfx_t + \frac{g^2(t)}{\sigma_t} \bseps_\theta(\bfx_t, \bfz, t)}_{= \bsf_\theta(\bfx_t,\bfz, t)}}\;\rmd t + g(t)\;\rmd\bar\bfw_t. \end{equation}\) and then by our result from Equation \eqref{eq:sde_is_ode} the adjoint diffusion SDE evolves with the following ODE \(\begin{equation} \frac{\rmd \bfa_\bfx}{\rmd t}(t) = -\bfa_\bfx(t)^\top \frac{\partial \bsf_\theta(\bfx_t, \bfz, t)}{\partial \bfx_t}. \end{equation}\)</p> <p>As the only difference between \(\bsf_\theta\) for diffusion SDEs and ODEs are a factor of 2 we realize that:</p> <blockquote> <p>We can use the <strong>same</strong> ODE solvers for adjoint diffusion SDEs!</p> </blockquote> <p>With the only caveat being the factor of 2. Therefore, we can modify the update equations from our code from above to now solve adjoint diffusion SDEs.</p> <figure class="highlight"><pre><code class="language-python" data-lang="python"><span class="n">adjoint_x</span> <span class="o">=</span> <span class="p">(</span><span class="n">alpha_t</span> <span class="o">/</span> <span class="n">alpha_s</span><span class="p">)</span> <span class="o">*</span> <span class="n">adjoint_x</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_s</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">expm1</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_t</span><span class="o">**</span><span class="mi">2</span> <span class="o">/</span> <span class="n">alpha_s</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">vec_J_xt</span>
<span class="n">adjoint_z</span> <span class="o">=</span> <span class="n">adjoint_z</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">sigma_s</span> <span class="o">*</span> <span class="n">torch</span><span class="p">.</span><span class="nf">expm1</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha_t</span> <span class="o">/</span> <span class="n">alpha_s</span><span class="p">)</span> <span class="o">*</span> <span class="n">vec_J_xt</span></code></pre></figure> <h2 id="concluding-remarks">Concluding remarks</h2> <p>This blog post gives a detailed introduction to the continuous adjoint equations. We discuss the theory behind them and why it is an appropriate tool for solving guided generation problems for diffusion models. This post serves as a summary for our recent NeurIPS paper:</p> <ul> <li><a href="https://openreview.net/forum?id=fAlcxvrOEX" rel="external nofollow noopener" target="_blank">Zander W. Blasingame and Chen Liu. <em>AdjointDEIS: Efficient Gradients for Diffusion Models</em>. NeurIPS 2024</a></li> </ul> <p>For examples of this technique used in practice check out our full paper and concurrent work from our colleagues <d-cite key="marion2024implicit,pan2024adjointdpm"></d-cite> which explore different experiements and focus on different aspects of implementing the continuous adjoint equations.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-11-20-adjointdeis.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Zander W. Blasingame. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"About",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-news",title:"News",description:"",section:"Navigation",handler:()=>{window.location.href="/news/"}},{id:"nav-blog",title:"Blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-publications",title:"Publications",description:"",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"CV",description:"",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-the-continuous-adjoint-equations-for-diffusion-models",title:"The Continuous Adjoint Equations for Diffusion Models",description:"This blog introduces the topic of the continuous adjoint equations for diffusion models, an efficient way to calculate gradients for diffusion models. We show how to design bespoke ODE/SDE solvers of the continuous adjoint equations and show that adjoint diffusion SDEs actually simplify to the adjoint diffusion ODE.",section:"Posts",handler:()=>{window.location.href="/blog/2024/adjointdeis/"}},{id:"post-face-morphing-with-diffusion-models",title:"Face Morphing with Diffusion Models",description:"This blog introduces a new family of face morphing attacks known as Difusion Morphs (DiM). DiMs are a novel method for constructing morphed faces which exploit the iterative nature of diffusion models to construct face morphs which are more effective and more realistic in appearance. DiMs achieve state-of-the-art morphing performance and visual fidelity, far surpassing previous methods. In this blog post I will detail the intution, basic concepts, and applications of DiMs.",section:"Posts",handler:()=>{window.location.href="/blog/2024/face-morphing-dim/"}},{id:"news-presented-our-work-on-adversarial-face-morphing-attack-detection-https-ieeexplore-ieee-org-document-9484383-at-ifpc-2020-https-www-nist-gov-news-events-events-2020-10-international-face-performance-conference-ifpc-2020",title:"Presented our work on [adversarial face morphing attack detection](https://ieeexplore.ieee.org/document/9484383) at [IFPC 2020](https://www.nist.gov/news-events/events/2020/10/international-face-performance-conference-ifpc-2020)",description:"",section:"News"},{id:"news-presented-our-paper-leveraging-adversarial-learning-for-the-detection-of-morphing-attacks-https-ieeexplore-ieee-org-document-9484383-at-ijcb-2021-https-ijcb2021-iapr-tc4-org",title:"Presented our paper [Leveraging Adversarial Learning for the Detection of Morphing Attacks](https://ieeexplore.ieee.org/document/9484383) at...",description:"",section:"News"},{id:"news-presented-our-work-on-face-morph-generation-at-ifpc-2022-https-www-nist-gov-news-events-events-2022-11-international-face-performance-conference-ifpc-2022",title:"Presented our work on face morph generation at [IFPC 2022](https://www.nist.gov/news-events/events/2022/11/international-face-performance-conference-ifpc-2022)",description:"",section:"News"},{id:"news-the-pre-print-on-dim-https-zblasingame-github-io-dim-is-now-available-on-arxiv-https-arxiv-org-abs-2301-04218",title:"The pre-print on [DiM](https://zblasingame.github.io/DiM/) is now available on [arXiv](https://arxiv.org/abs/2301.04218)",description:"",section:"News"},{id:"news-presented-our-work-on-diffusion-morphs-https-zblasingame-github-io-dim-eab-amp-citer-biometrics-workshop-https-eab-org-events-program-312-in-martigny-switzerland",title:"Presented our work on [Diffusion Morphs](https://zblasingame.github.io/DiM/) @ [EAB &amp; CITeR Biometrics Workshop](https://eab.org/events/program/312) in...",description:"",section:"News"},{id:"news-the-pre-print-on-fast-dim-https-zblasingame-github-io-dim-is-now-available-on-arxiv-https-arxiv-org-abs-2310-09484",title:"The pre-print on [Fast-DiM](https://zblasingame.github.io/DiM/) is now available on [arXiv](https://arxiv.org/abs/2310.09484)",description:"",section:"News"},{id:"news-our-paper-leveraging-diffusion-for-strong-and-high-quality-face-morphing-attacks-https-zblasingame-github-io-dim-was-accepted-in-the-ieee-transactions-on-biometrics-behavior-and-identity-science-https-ieeexplore-ieee-org-document-10381591",title:"Our paper *[Leveraging Diffusion for Strong and High Quality Face Morphing Attacks](https://zblasingame.github.io/DiM/)* was...",description:"",section:"News"},{id:"news-presented-a-talk-on-diffusion-morphs-to-the-document-security-alliance-https-documentsecurityalliance-org-slides-assets-pdf-citer-dsa-spring-2024-pdf",title:"Presented a talk on Diffusion Morphs to the [Document Security Alliance](https://documentsecurityalliance.org/) [(slides)](assets/pdf/citer_dsa_spring_2024.pdf)",description:"",section:"News"},{id:"news-the-pre-print-on-greedy-dim-https-zblasingame-github-io-greedy-dim-is-now-available-on-arxiv-https-arxiv-org-abs-2404-06025",title:"The pre-print on [Greedy-DiM](https://zblasingame.github.io/Greedy-DiM/) is now available on [arXiv](https://arxiv.org/abs/2404.06025)",description:"",section:"News"},{id:"news-our-pre-print-the-impact-of-print-and-scan-in-heterogeneous-morph-evaluation-scenarios-https-arxiv-org-abs-2404-06559-is-now-available-on-arxiv-https-arxiv-org-abs-2404-06025",title:"Our pre-print *[The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios](https://arxiv.org/abs/2404.06559)* is now...",description:"",section:"News"},{id:"news-the-pre-print-on-adjointdeis-a-novel-method-for-calculating-gradients-for-diffusion-odes-sdes-is-now-available-on-arxiv-https-arxiv-org-abs-2405-15020",title:"The pre-print on *AdjointDEIS*, a novel method for calculating gradients for diffusion ODEs/SDEs,...",description:"",section:"News"},{id:"news-our-paper-fast-dim-towards-fast-diffusion-morphs-https-zblasingame-github-io-dim-was-accepted-in-the-ieee-security-amp-privacy-https-ieeexplore-ieee-org-document-10569993",title:"Our paper *[Fast-DiM: Towards Fast Diffusion Morphs](https://zblasingame.github.io/DiM/)* was accepted in the [IEEE Security...",description:"",section:"News"},{id:"news-our-paper-greedy-dim-greedy-algorithms-for-unreasonably-effective-face-morphs-https-zblasingame-github-io-greedy-dim-was-accepted-into-the-main-track-ijcb-2024-https-ijcb2024-ieee-biometrics-org",title:"Our paper *[Greedy-DiM: Greedy Algorithms for Unreasonably Effective Face Morphs](https://zblasingame.github.io/Greedy-DiM/)* was accepted into...",description:"",section:"News"},{id:"news-presented-a-talk-on-diffusion-morphs-to-the-biometrics-security-amp-privacy-group-https-www-idiap-ch-en-scientific-research-biometrics-security-and-privacy-idiap-https-www-idiap-ch-in-martigny-switzerland-slides-assets-pdf-idiap-talk-2024-pdf",title:"Presented a talk on Diffusion Morphs to the [Biometrics Security &amp; Privacy Group](https://www.idiap.ch/en/scientific-research/biometrics-security-and-privacy)...",description:"",section:"News"},{id:"news-attended-differentiable-almost-everything-https-differentiable-xyz-icml-2024-in-vienna",title:"Attended [Differentiable Almost Everything](https://differentiable.xyz/) @ ICML 2024 in Vienna",description:"",section:"News"},{id:"news-our-paper-the-impact-of-print-and-scan-in-heterogeneous-morph-evaluation-scenarios-https-arxiv-org-abs-2404-06559-was-accepted-into-the-fmadt-2024-https-sites-google-com-clarkson-edu-fmadt2024-home-special-session-ijcb-2024-https-ijcb2024-ieee-biometrics-org",title:"Our paper *[The Impact of Print-and-Scan in Heterogeneous Morph Evaluation Scenarios](https://arxiv.org/abs/2404.06559)* was accepted...",description:"",section:"News"},{id:"news-i-was-accepted-into-the-doctoral-consortium-at-ijcb-2024-https-ijcb2024-ieee-biometrics-org",title:"I was accepted into the Doctoral Consortium at @ [IJCB 2024](https://ijcb2024.ieee-biometrics.org/)",description:"",section:"News"},{id:"news-our-ieee-tbiom-https-ieeexplore-ieee-org-document-10381591-paper-leveraging-diffusion-for-strong-and-high-quality-face-morphing-attacks-was-accepted-into-the-journal-track-ijcb-2024-https-ijcb2024-ieee-biometrics-org",title:"Our [IEEE TBIOM](https://ieeexplore.ieee.org/document/10381591) paper *Leveraging Diffusion For Strong and High Quality Face Morphing...",description:"",section:"News"},{id:"news-our-paper-towards-effective-machine-learning-models-for-ransomware-detection-via-low-level-hardware-information-was-accepted-hasp-2024-https-haspworkshop-org-2024",title:"Our paper *Towards Effective Machine Learning Models for Ransomware Detection via Low-Level Hardware...",description:"",section:"News"},{id:"news-presented-three-of-our-papers-ijcb-2024",title:"Presented three of our papers @ IJCB 2024",description:"",section:"News",handler:()=>{window.location.href="/news/ijcb_2024/"}},{id:"news-our-paper-adjointdeis-efficient-gradients-for-diffusion-models-https-arxiv-org-abs-2405-15020-was-accepted-in-the-main-track-neurips-2024-https-neurips-cc",title:"Our paper *[AdjointDEIS: Efficient Gradients for Diffusion Models](https://arxiv.org/abs/2405.15020)* was accepted in the main...",description:"",section:"News"},{id:"news-chutitep-woralert-https-scholar-google-com-citations-user-86btx0caaaaj-presented-our-paper-towards-effective-machine-learning-models-for-ransomware-detection-via-low-level-hardware-information-hasp-2024-https-haspworkshop-org-2024",title:"[Chutitep Woralert](https://scholar.google.com/citations?user=86BtX0cAAAAJ) presented our paper *Towards Effective Machine Learning Models for Ransomware Detection...",description:"",section:"News"},{id:"news-presented-a-talk-on-diffusion-morphs-at-the-transatlantic-dialogue-on-presentation-attack-detection-organized-by-the-european-association-for-biometrics-https-eab-org-eab-and-the-imars-project-https-imars-project-eu-in-washington-d-c-slides-assets-pdf-eab-dc-talk-2024-pdf",title:"Presented a talk on Diffusion Morphs at the Transatlantic Dialogue on Presentation Attack...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%62%6C%61%73%69%6E%7A%77@%63%6C%61%72%6B%73%6F%6E.%65%64%75","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0002-9508-8425","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=gBBtH3AAAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/46241552","_blank")}},{id:"socials-ieee-xplore",title:"IEEE Xplore",section:"Socials",handler:()=>{window.open("https://ieeexplore.ieee.org/author/37088915308/","_blank")}},{id:"socials-acm-dl",title:"ACM DL",section:"Socials",handler:()=>{window.open("https://dl.acm.org/profile/99659274639/","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/zblasingame","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/zander-blasingame","_blank")}},{id:"socials-dblp",title:"DBLP",section:"Socials",handler:()=>{window.open("https://dblp.org/pid/250/5790.html","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>